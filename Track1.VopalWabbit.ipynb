{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model, test and predict using Vopal Vabbit library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find d3_categories, viewed by users in prediction week, but not previous three ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib import train_calendar\n",
    "from classeslib import persistence_files\n",
    "from classeslib.predictions_users import PredictionUser, SilentWeeksReader, FutureWeekReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silent_weeks_files = [persistence_files.days_split_catalog + \"train.\" + str(day) + \".csv\" \n",
    "                      for day in train_calendar.target_silent_weeks]\n",
    "silent_weeks_reader = SilentWeeksReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for day_file in silent_weeks_files:\n",
    "    silent_weeks_reader.read_input_file(day_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794168\n"
     ]
    }
   ],
   "source": [
    "silent_weeks_users = silent_weeks_reader.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_week_files = [persistence_files.days_split_catalog + \"train.\" + str(day) + \".csv\" \n",
    "                     for day in train_calendar.target_week]\n",
    "target_week_reader = FutureWeekReader(silent_weeks_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for day_file in target_week_files:\n",
    "    target_week_reader.read_input_file(day_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_users = {user_id: user for (user_id, user) in target_week_reader.users.items() \n",
    "                    if len(user.silent_weeks_d3_categories) > 0 and len(user.target_categories) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.statistics import StatisticsDumper\n",
    "from classeslib import persistence_files\n",
    "prediction_users_interests = StatisticsDumper.get_users_interests(\n",
    "    prediction_users.keys(),\n",
    "    persistence_files.public_train_statistics_db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore clusters info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib import persistence_files\n",
    "from classeslib.clusterization import UsersClaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(persistence_files.cluster_model_file, \"r\") as cluster_file:\n",
    "    clusterization = pickle.load(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print sorted(set(clusterization.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shelve\n",
    "cluster_categories = {}\n",
    "clusters_categories_db = shelve.open(persistence_files.clusters_categories_db_file)\n",
    "for key in clusters_categories_db.keys():\n",
    "    cluster_categories[key] = clusters_categories_db[key]\n",
    "clusters_categories_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print persistence_files.clusters_categories_db_file\n",
    "print sorted(cluster_categories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n"
     ]
    }
   ],
   "source": [
    "(d1_level_statistics,\n",
    " d2_level_statistics,\n",
    " d3_level_statistics) = StatisticsDumper.restore_statistics(persistence_files.public_train_statistics_db_file)\n",
    "\n",
    "d1_categories_sorted = sorted(d1_level_statistics.categories_statistics.keys(),\n",
    "                       key = lambda c : d1_level_statistics.categories_statistics[c],\n",
    "                       reverse=True)\n",
    "d2_categories_sorted = sorted(d2_level_statistics.categories_statistics.keys(),\n",
    "                       key = lambda c : d2_level_statistics.categories_statistics[c],\n",
    "                       reverse=True)\n",
    "d3_categories_sorted = sorted(d3_level_statistics.categories_statistics.keys(),\n",
    "                       key = lambda c : d3_level_statistics.categories_statistics[c],\n",
    "                       reverse=True)\n",
    "print len(d3_categories_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form train dataset for Vopal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.features import build_categories_features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "def find_negative_sample(user_cluster, d3_categories, positive_categories):\n",
    "    d3_categories_count = len(d3_categories)\n",
    "    cluster_d3_categories = cluster.d3_categories_popularity.keys()\n",
    "    start_category_index = random.randint(0, d3_categories_count)\n",
    "    index = start_category_index\n",
    "    while True:\n",
    "        d3_category = d3_categories[index]\n",
    "        if d3_category not in cluster_d3_categories and d3_category not in positive_categories:\n",
    "            return d3_category\n",
    "        index = (index + 1) % d3_categories_count\n",
    "        if index == start_category_index:\n",
    "            break\n",
    "    \n",
    "    without_positive_categories = list(set(cluster_d3_categories) - set(positive_categories))\n",
    "    if len(without_positive_categories) < 4 :\n",
    "        return without_positive_categories[len(without_positive_categories)-1]\n",
    "    \n",
    "    last_third_index = 3 * len(without_positive_categories) / 4\n",
    "    least_popular_category_index = random.randint(last_third_index, len(without_positive_categories) - 1)\n",
    "    return without_positive_categories[least_popular_category_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_class = 1\n",
    "negative_class = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_train_file = \"./vw_train_users_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_user_cluster(user_id, user_interests, cluster_model, clusters_categories):\n",
    "    user_features = build_categories_features_matrix(\n",
    "        d1_categories_sorted, d2_categories_sorted, d3_categories_sorted, {user_id: user_interests})[0]\n",
    "    return cluster_model.predict(user_features)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.vw_datasets import VwDatasetWriter\n",
    "from tqdm import tqdm\n",
    "#from imp import reload\n",
    "#import classeslib.vw_datasets\n",
    "#VwDatasetWriter = reload(classeslib.vw_datasets)\n",
    "#from classeslib.vw_datasets import VwDatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339657/339657 [1:45:27<00:00, 53.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(vw_train_file, 'w') as train_dataset:\n",
    "    \n",
    "    dataset_writer = VwDatasetWriter(train_dataset)\n",
    "    \n",
    "    for (user_id, user) in tqdm(prediction_users.items()):  \n",
    "        #print user_id\n",
    "        silent_week_categories = ' '.join(user.silent_weeks_d3_categories)\n",
    "        if silent_week_categories == \"\":\n",
    "            print user_id, \"no silent weeks categories\"\n",
    "            continue\n",
    "            \n",
    "        cluster_label = find_user_cluster(\n",
    "            user_id, prediction_users_interests[user_id], clusterization, cluster_categories)\n",
    "        \n",
    "        #if not cluster_categories.has_key(cluster_label):\n",
    "        #    continue\n",
    "            \n",
    "        cluster = cluster_categories[str(cluster_label)]\n",
    "        \n",
    "         # write \"positive\" samples (categories form silent week) -> viewed category\n",
    "        for target_category in user.target_categories:   \n",
    "            dataset_writer.write(positive_class, user_id, target_category, silent_week_categories)\n",
    "            \n",
    "         # write \"negative\" samples (categories form silent week) -> not viewed category    \n",
    "        for s in range(0, 5):\n",
    "            negative_sample = find_negative_sample(cluster, d3_categories_sorted, user.target_categories)\n",
    "            dataset_writer.write(negative_class, user_id, negative_sample, silent_week_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Vopal Wabbit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OSError: [Errno 12] Cannot allocate memory - run from separate console\n",
    "#rm vw_train_users_dataset.cache\n",
    "#vw -d vw_train_users_dataset -c --passes 2 -f vw.track1_model -q DC --quiet --binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test week users and their categories in test week and three weeks before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_silent_weeks_files = [persistence_files.days_split_catalog + \"train.\" + str(day) + \".csv\" \n",
    "                      for day in train_calendar.test_silent_weeks]\n",
    "test_silent_weeks_reader = SilentWeeksReader()\n",
    "\n",
    "for day_file in test_silent_weeks_files:\n",
    "    test_silent_weeks_reader.read_input_file(day_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_silent_weeks_users = silent_weeks_reader.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_week_files = [persistence_files.days_split_catalog + \"train.\" + str(day) + \".csv\" \n",
    "                     for day in train_calendar.test_week]\n",
    "test_week_reader = FutureWeekReader(test_silent_weeks_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for day_file in test_week_files:\n",
    "    test_week_reader.read_input_file(day_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_week_users = {user_id: user for (user_id, user) in test_week_reader.users.items() \n",
    "                    if len(user.silent_weeks_d3_categories) > 0 and len(user.target_categories) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.statistics import StatisticsDumper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_users_interests = StatisticsDumper.get_users_interests(\n",
    "    test_week_users.keys(),\n",
    "    persistence_files.public_train_statistics_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010997\n",
      "304953\n"
     ]
    }
   ],
   "source": [
    "print len(test_week_reader.users)\n",
    "print len(test_users_interests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form test dataset for Vopal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_test_file = \"./vw_test_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.candidates import get_candidate_categories\n",
    "#from imp import reload\n",
    "#import classeslib\n",
    "#reload(classeslib.candidates)\n",
    "#from classeslib.candidates import get_candidate_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(vw_test_file, 'w') as test_dataset:\n",
    "    \n",
    "    dataset_writer = VwDatasetWriter(test_dataset)\n",
    "    \n",
    "    for user_id in test_users_interests.keys():        \n",
    "        \n",
    "        user = test_week_users[user_id]\n",
    "        silent_week_categories = ' '.join(user.silent_weeks_d3_categories)\n",
    "        if silent_week_categories == \"\":\n",
    "            continue\n",
    "            \n",
    "        cluster_label = find_user_cluster(\n",
    "            user_id, test_users_interests[user_id], clusterization, cluster_categories)  \n",
    "        \n",
    "        cluster = cluster_categories[str(cluster_label)]\n",
    "            \n",
    "        candidates = get_candidate_categories(cluster, user_id, user, test_users_interests, d3_categories_sorted)    \n",
    "        \n",
    "        # write candidates rows to test dataset\n",
    "        for candidate in candidates:\n",
    "            dataset_writer.write(1, user_id, candidate, silent_week_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run VW on test dataset in test mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OSError: [Errno 12] Cannot allocate memory - run in separate terminal\n",
    "#!vw -i vw.track1_model -t vw_test_dataset -p vw_test_dataset.out --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate test predictions score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.candidates import get_users_predicted_categories \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = dict((user_id, list(test_week_users[user_id].target_categories)) for user_id in test_week_users.keys())\n",
    "num_users = len(y_true)\n",
    "\n",
    "y_pred = get_users_predicted_categories(vw_test_file, \"./vw_test_dataset.out\", d3_categories_sorted)\n",
    "num_users_5p = int(np.ceil(0.05 * num_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329451\n",
      "16473\n",
      "304953\n"
     ]
    }
   ],
   "source": [
    "print (num_users)\n",
    "print (num_users_5p)\n",
    "print len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_active_test_users = sorted(\n",
    "    y_pred.items(),\n",
    "    key = lambda (user_id, u): test_week_users[user_id].views,\n",
    "    reverse = True)[:num_users_5p]\n",
    "\n",
    "y_pred = dict(most_active_test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2477.994294\n"
     ]
    }
   ],
   "source": [
    "#from scorer import scorer\n",
    "#score = scorer(y_true, y_pred, num_users)\n",
    "#print \"Score: %f\" % score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
