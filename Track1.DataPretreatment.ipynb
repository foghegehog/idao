{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to separate days files for convenience of handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting reader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.input_file_reader import InputFileReader\n",
    "\n",
    "class SplitFileReader(InputFileReader):\n",
    "    def __init__(self, days_split_catalog):\n",
    "        self.days_split_catalog = days_split_catalog\n",
    "        self.days_file_handlers = {}\n",
    "        \n",
    "    def close(self):\n",
    "        for file_handler in self.days_file_handlers.values():\n",
    "            file_handler.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_file_name(day):\n",
    "        return \"train.\" + str(day) + \".csv\"\n",
    "    \n",
    "    def handle_data_row(self, user_id, day, d1_category, d2_category, d3_category):\n",
    "        if self.days_file_handlers.has_key(day):\n",
    "            day_file_handler = self.days_file_handlers[day]\n",
    "        else:\n",
    "            day_file_path = self.days_split_catalog + SplitFileReader.format_file_name(day)\n",
    "            day_file_handler = open(day_file_path, 'w')\n",
    "            day_file_handler.write(self.header)\n",
    "            self.days_file_handlers[day] = day_file_handler\n",
    "        line = InputFileReader.format_line(user_id, day, d1_category, d2_category, d3_category)    \n",
    "        day_file_handler.write(line) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test file spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test splitting - PASSED\n"
     ]
    }
   ],
   "source": [
    "# arrange\n",
    "test_split_filepath = \"./tests_data/unittest.train.csv\"\n",
    "test_split_catalog = \"./tests_data/unittest.train.split/\"\n",
    "\n",
    "with open(test_split_filepath, \"w\") as test_file:\n",
    "    test_file.write(\"id3,user_id,id2,date,id1\\n\")\n",
    "    test_file.write(\"111,1,11,1,1\\n\")\n",
    "    test_file.write(\"112,2,11,1,1\\n\")\n",
    "    test_file.write(\"121,3,12,3,1\\n\")\n",
    "    test_file.write(\"122,4,12,5,1\\n\")\n",
    "    test_file.write(\"211,1,21,3,2\\n\")\n",
    "    test_file.write(\"212,2,21,4,2\\n\")\n",
    "    test_file.write(\"221,3,22,3,2\\n\")\n",
    "    test_file.write(\"222,4,22,1,2\\n\")\n",
    "    test_file.write(\"311,1,31,3,3\\n\")\n",
    "    test_file.write(\"312,2,31,4,3\\n\")\n",
    "    test_file.write(\"321,3,32,3,3\\n\")\n",
    "    test_file.write(\"322,4,32,1,3\\n\")\n",
    "\n",
    "expected_daily_views = {\"1\":4, \"3\":5, \"4\":2, \"5\":1}\n",
    "\n",
    "class SplitterTest(InputFileReader):\n",
    "    def __init__(self, day):\n",
    "        self.day = day\n",
    "        self.views = 0\n",
    "        \n",
    "    def handle_data_row(self, user_id, day, d1_category, d2_category, d3_category):\n",
    "        assert day == self.day, \"day \" + day + \"is diferent from \" + self.day\n",
    "        self.views += 1\n",
    "\n",
    "testers = [SplitterTest(day) for day in expected_daily_views.keys()]        \n",
    "    \n",
    "split_reader = SplitFileReader(test_split_catalog)\n",
    "\n",
    "# act\n",
    "split_reader.read_input_file(test_split_filepath)    \n",
    "split_reader.close()\n",
    "\n",
    "# assert\n",
    "import os.path\n",
    "\n",
    "for tester in testers:\n",
    "    filename = SplitFileReader.format_file_name(tester.day)\n",
    "    filepath = test_split_catalog + filename\n",
    "    assert os.path.isfile(filepath), \"file not found: \" + filepath\n",
    "    tester.read_input_file(filepath)\n",
    "    assert tester.header == split_reader.header, \"wrong file header for day \" + tester.day\n",
    "    assert tester.views == expected_daily_views[tester.day], \"wrong amount of views on day \" + tester.day\n",
    "    \n",
    "print (\"Test splitting - PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split real input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"./train.csv\"\n",
    "from classeslib import persistence_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_reader = SplitFileReader(persistence_files.days_split_catalog)\n",
    "\n",
    "# act\n",
    "split_reader.read_input_file(train_filepath)    \n",
    "split_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather statistics about users's views and popular categories "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate statistics and save it to file db with periodical dump due to memory usage (pandas is terra incognita yet, unfortunatelly) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classeslib.statistics import StatisticsDumper, StatisticsCounter\n",
    "from classeslib import train_calendar\n",
    "from classeslib import persistence_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dumper = StatisticsDumper(persistence_files.public_train_statistics_db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather statistics from days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in train_calendar.public_train_days:\n",
    "    day_file_path = persistence_files.days_split_catalog + SplitFileReader.format_file_name(day)\n",
    "    counter = StatisticsCounter(100000, day_file_path, train_dumper) \n",
    "    print \"handling file \", day_file_path\n",
    "    counter.calculate_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(d1_level_statistics,\n",
    " d2_level_statistics,\n",
    " d3_level_statistics) = train_dumper.restore_statistics(persistence_files.public_train_statistics_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n"
     ]
    }
   ],
   "source": [
    "print len(d3_level_statistics.categories_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
